Here is my new training step:


    @tf.function

    def train_step(self, state, action, reward, next_state, done):

        """Performs a single training step with entropy regularization."""

        with tf.GradientTape() as tape:

            action_logits, state_value = self.model(state)

            _, next_state_value = self.model(next_state)


            # Critic loss

            state_value = tf.squeeze(state_value)

            next_state_value = tf.squeeze(next_state_value)


            td_target = reward + self.gamma * next_state_value * (1 - tf.cast(done, tf.float32))

            advantage = td_target - state_value

            critic_loss = self.huber_loss(tf.expand_dims(td_target, 0), tf.expand_dims(state_value, 0))


            # Actor loss with entropy regularization

            action_indices = tf.stack([tf.range(state.shape[0], dtype=tf.int32), tf.cast(action, tf.int32)], axis=1)

            log_probs = tf.nn.log_softmax(action_logits)

            action_log_probs = tf.gather_nd(log_probs, action_indices)

            

            # Entropy bonus for exploration

            action_probs = tf.nn.softmax(action_logits)

            entropy = -tf.reduce_sum(action_probs * log_probs, axis=-1)

            

            actor_loss = -action_log_probs * tf.stop_gradient(advantage) - self.entropy_coef * entropy


            total_loss = actor_loss + critic_loss


        grads = tape.gradient(total_loss, self.model.trainable_variables)

        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))

        

        # Return metrics for monitoring

        return {

            'actor_loss': actor_loss,

            'critic_loss': critic_loss,

            'advantage': advantage,

            'entropy': entropy,

            'td_error': tf.abs(advantage)

        }


my old:



    @tf.function

    def train_step(self, state, action, reward, next_state, done):

        """Performs a single training step for both actor and critic."""

        with tf.GradientTape() as tape:

            # Get model outputs for current and next states

            action_logits, state_value = self.model(state)

            _, next_state_value = self.model(next_state)


            # Critic loss calculation

            state_value = tf.squeeze(state_value)

            next_state_value = tf.squeeze(next_state_value)


            td_target = reward + self.gamma * next_state_value * (1 - float(done))

            advantage = td_target - state_value

            critic_loss = self.huber_loss(tf.expand_dims(td_target, 0), tf.expand_dims(state_value, 0))


            # Actor loss calculation (policy gradient)

            # The 'action' tensor (from numpy) defaults to int64, while tf.range defaults to int32.

            # tf.stack requires all input tensors to have the same dtype. We cast 'action' to int32 to resolve this.

            action_indices = tf.stack([tf.range(state.shape[0], dtype=tf.int32), tf.cast(action, tf.int32)], axis=1)

            log_probs = tf.nn.log_softmax(action_logits)

            action_log_probs = tf.gather_nd(log_probs, action_indices)

            

            # Stop gradient to treat advantage as a constant

            actor_loss = -action_log_probs * tf.stop_gradient(advantage)


            # Total loss (you can weight these if needed)

            total_loss = actor_loss + critic_loss


        # Compute and apply gradients

        grads = tape.gradient(total_loss, self.model.trainable_variables)

        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))

        

        return actor_loss, critic_loss


past js version (with clipping):



        async function trainActorCritic(state, action, reward, nextState, done) {

            // const stateTensor = tf.tensor2d([state]);

            // const nextStateTensor = tf.tensor2d([nextState]);

            const stateTensor = state; // state is already a tensor

            const nextStateTensor = nextState; // nextState is already a tensor

            

            // Get current and next state values

            const currentValue = valueNetwork.predict(stateTensor);

            const nextValue = done ? tf.zeros([1, 1]) : valueNetwork.predict(nextStateTensor);

            

            const currentValueData = currentValue.dataSync()[0];

            const nextValueData = nextValue.dataSync()[0];

            

            // Calculate TD target and advantage with clipping

            const tdTarget = reward + gamma * nextValueData;

            const advantage = Math.max(-5, Math.min(5, tdTarget - currentValueData));

            

            currentAdvantage = advantage;

            

            // Train value network and track gradient

            const targetTensor = tf.tensor2d([[tdTarget]]);

            let valueGradMagnitude = 0;

            

            // Calculate value network gradients

            const valueGrads = tf.variableGrads(() => {

                const pred = valueNetwork.apply(stateTensor);

                return tf.losses.meanSquaredError(targetTensor, pred);

            });

            

            // Calculate gradient magnitude for value network

            valueGradMagnitude = Object.values(valueGrads.grads).reduce((sum, grad) => {

                return sum + tf.norm(grad).dataSync()[0];

            }, 0);

            

            const valueFitHistory = await valueNetwork.fit(stateTensor, targetTensor, { 

                epochs: 1, 

                verbose: 0,

                validationSplit: 0 

            });

            const valueLoss = valueFitHistory.history.loss[0];

            

            // Train policy network with gradient tracking

            const actionTensor = tf.tensor1d([action], 'int32');

            let policyLossValue = 0;

            let policyGradMagnitude = 0;

            

            // const optimizer = tf.train.adam(learningRate * 0.1); 

            const optimizer = tf.train.adam(learningRate); 

            

            const f = () => tf.tidy(() => {

                const logits = policyNetwork.apply(stateTensor);

                const probs = tf.softmax(logits);

                const logProbs = tf.log(tf.clipByValue(probs, 1e-8, 1 - 1e-8));

                const actionMask = tf.oneHot(actionTensor, numActions);

                const pickedLogProb = tf.sum(tf.mul(logProbs, actionMask));

                

                const clippedAdvantage = tf.clipByValue(tf.scalar(advantage), -5, 5);

                const loss = tf.neg(tf.mul(pickedLogProb, clippedAdvantage));

                

                policyLossValue = loss.dataSync()[0];

                return loss;

            });

            

            // Calculate policy gradients before optimization

            const policyGrads = tf.variableGrads(f);

            policyGradMagnitude = Object.values(policyGrads.grads).reduce((sum, grad) => {

                return sum + tf.norm(grad).dataSync()[0];

            }, 0);

            

            await optimizer.minimize(f);

            optimizer.dispose();

   

            // Store gradient information

            gradientHistory.push({

                step: trainingSteps,

                policyGradMag: policyGradMagnitude,

                valueGradMag: valueGradMagnitude,

                policyLoss: policyLossValue,

                valueLoss: valueLoss

            });

            

            // Keep gradient history manageable

            if (gradientHistory.length > 1000) {

                gradientHistory.shift();

            }

            

            // Store losses with NaN check

            if (!isNaN(policyLossValue) && !isNaN(valueLoss)) {

                policyLossHistory.push(policyLossValue);

                valueLossHistory.push(valueLoss);

                updateLossCharts();

            }

            

            trainingSteps++;

            

            // Cleanup tensors

            Object.values(valueGrads.grads).forEach(grad => grad.dispose());

            Object.values(policyGrads.grads).forEach(grad => grad.dispose());

            // stateTensor.dispose();

            // nextStateTensor.dispose();

            currentValue.dispose();

            nextValue.dispose();

            targetTensor.dispose();

            actionTensor.dispose();

        }