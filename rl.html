<!DOCTYPE html>
<html lang="en">
<head>
    <title>Nets vs. Automata: RL @ RC</title>
    <link rel="stylesheet" href="./style.css">
    <link rel="icon" type="image/x-icon" href="./nva_logo.ico">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        .main-box2 {
            width: 90%;
            height: 85vh;
            margin: 10px auto 0;
            background-color: rgb(1, 44, 5);
            border-radius: 10px;
            overflow: auto;
            display: flex;
            flex-direction: column;
        }
        .target-overlay {
            position: absolute;
            top: 0;
            left: 0;
            pointer-events: none;
            border: 3px solid #00ff00;
            background: rgba(0, 255, 0, 0.3);
            transition: all 0.2s ease;
        }

        .rl-container {
            display: flex;
            flex-direction: column;
            gap: 20px;
            padding: 20px;
            align-items: center;
        }
        
        .controls-section {
            width: 100%;
            max-width: 800px;
        }
        
        .controls-row {
            display: flex;
            gap: 15px;
            flex-wrap: wrap;
            /* align-items: center; */
            /* align-items: flex-start; */
            justify-content: center;
        }
        
        .control-group {
            display: flex;
            flex-direction: column;
            gap: 5px;
        }
        
        .control-group label {
            font-size: 12px;
            font-weight: bold;
        }
        
        .control-group select {
            padding: 5px;
            border-radius: 4px;
            border: 1px solid #ccc;
        }
        
        .visualization-container {
            display: flex;
            flex-direction: column;
            gap: 20px;
            /* align-items: flex-start; */
            align-items: center;
            flex-wrap: wrap;
            justify-content: center;
        }
        
        .ca-display {
            border: 2px solid #333;
            position: relative;
        }

        .pattern-display {
            border: 2px solid #333;
            /* position: relative; */
        }
        
        .agent-overlay {
            position: absolute;
            top: 0;
            left: 0;
            pointer-events: none;
            border: 3px solid #ff0000;
            background: rgba(255, 0, 0, 0.2);
            transition: all 0.2s ease;
        }
        
        .stats-panel {
            background: #1f3f27;
            padding: 10px;
            border-radius: 8px;
            min-width: 200px;
            max-width: 300px;
        }
        
        .stats-panel h4 {
            margin-bottom: 0;
            color: #b8a2a2;
        }

        #stateRepresentation {
            flex: 1 1 auto;          /* allow it to shrink/grow */
            min-width: 0;            /* critical for wrapping in flex */
            max-width: 100%;
            white-space: normal;
            overflow-wrap: anywhere; /* or: word-break: break-all; */
        }
        
        .metric-row {
            display: flex;
            justify-content: space-between;
            margin: 5px 0;
        }
        
        .action-buttons {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
            justify-content: center;
        }
        
        .action-btn {
            padding: 8px 12px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
            transition: background-color 0.2s;
        }
        
        .action-btn:hover {
            opacity: 0.8;
        }
        
        .train-btn { background: #4CAF50; color: white; }
        .step-btn { background: #2196F3; color: white; }
        .reset-btn { background: #ff9800; color: white; }
        .dimension-btn { background: #607d8b; color: white; }
        
        .charts-container {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
            justify-content: center;
        } 
        
        .chart-wrapper {
            background: rgb(191, 252, 186);
            padding: 10px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .progress-bar {
            width: 100%;
            height: 20px;
            background: #f0f0f0;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }
        
        .progress-fill {
            height: 100%;
            background: linear-gradient(90deg, #4CAF50, #45a049);
            width: 0%;
            transition: width 0.3s ease;
        }

        .action-probs-chart {
            display: flex;
            gap: 2px;
            margin: 5px 0;
            height: 20px;
            background: #f0f0f0;
            border-radius: 3px;
            overflow: hidden;
        }

        .action-bar {
            background: #2196F3;
            transition: width 0.3s ease;
            position: relative;
        }

        .action-bar:hover::after {
            content: attr(data-action);
            position: absolute;
            bottom: 25px;
            left: 50%;
            transform: translateX(-50%);
            background: #333;
            color: white;
            padding: 2px 4px;
            border-radius: 2px;
            font-size: 10px;
            white-space: nowrap;
        }

        .separate-loss-charts {
            display: flex;
            /* flex-direction: column; */
            gap: 20px;
            flex-wrap: wrap;
            justify-content: center;
        }

        .torus-view {
            background: #000;
            border-radius: 8px;
            margin: 10px 0;
        }

        .mode-toggle-section {
            display: flex;
            justify-content: center;
            margin: 20px 0;
        }

        .mode-selector {
            display: flex;
            background: #333;
            border-radius: 8px;
            padding: 4px;
            gap: 4px;
        }

        .mode-btn {
            padding: 12px 24px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-weight: bold;
            background: transparent;
            color: #ccc;
            transition: all 0.3s ease;
        }

        .mode-btn:hover {
            background: #555;
            color: #fff;
        }

        .mode-btn.active {
            background: #4CAF50;
            color: white;
        }

        /* Update existing combined controls to show/hide based on mode */
        .manual-mode-only {
            display: flex;
        }

        .rl-mode-only {
            display: none;
        }

        .rl-mode .manual-mode-only {
            display: none;
        }

        body.rl-mode .rl-mode-only {
            display: flex;
        }

        #patternCanvas {
            background-image: 
                linear-gradient(to right, #666 1px, transparent 1px),
                linear-gradient(to bottom, #666 1px, transparent 1px);
            background-size: 25px 25px;
            border: 1px solid #666;
        }

        #rlPatternCanvas {
            background-image: 
                linear-gradient(to right, #666 1px, transparent 1px),
                linear-gradient(to bottom, #666 1px, transparent 1px);
            background-size: 15px 15px;
            border: 1px solid #666;
        }

        @media (max-width: 768px) {
            .rl-container {
                padding: 10px;
            }
            
            .visualization-container {
                flex-direction: column;
                align-items: center;
            }
            
            .controls-row {
                justify-content: center;
            }
            
            .charts-container {
                flex-direction: column;
                align-items: center;
            }
            
            .chart-wrapper {
                width: 100%;
                max-width: 400px;
            }

            .stop-btn { 
                background: #f44336; 
                color: white; 
            }

            .stop-btn:hover {
                background: #d32f2f;
            }

            /* Ensure button layouts are consistent */
            #manualButtons, #rlButtons {
                display: flex;
                gap: 10px;
                flex-wrap: wrap;
                justify-content: center;
            }
        }
    </style>
</head>
<body>
    <div id="title-wrapper">
      <nav>
          <div class="menu-container">
                  <div class="menu-toggle">
                  <div class="hamburger"></div>
                  <div class="hamburger"></div>
                  <div class="hamburger"></div>
              </div>
              <ul class="menu">
                  <li><a href="index.html">Training</a></li>
                  <li><a href="analysis.html">Analysis</a></li>
                  <li><a href="forum.html">Forum</a></li>
                  <li><a href="profile.html">Profile</a></li>
                  <li><a href="about.html">About</a></li>
              </ul>
          </div>
      </nav>
      <a href="index.html" class="title-link">
          <h2 class="title">Netsüï∏Ô∏èvsüëæAutomata</h2>
      </a>
    </div>
    
    <div class="main-box2">
      <div class="content">
        <div class="rl-container">
            <!-- <h1>Reinforcement Learning @ Recurse Center</h1> -->

            <div class="controls-section">
                <div class="controls-row">
                    <div class="control-group">
                        <label>Grid Size:</label>
                        <select id="gridSize">
                            <option value="12" selected>12x12</option>
                            <option value="16">16x16</option>
                            <option value="20">20x20</option>
                        </select>
                    </div>
                    
                    <div class="control-group">
                        <label>Initial Density:</label>
                        <select id="initialDensity">
                            <option value="0.1">10%</option>
                            <option value="0.2">20%</option>
                            <option value="0.3">30%</option>
                            <option value="0.4" selected>40%</option>
                            <option value="0.5">50%</option>
                        </select>
                    </div>
                    
                    <div class="control-group">
                        <label>CA Rules:</label>
                        <select id="caRules">
                            <option value="conway" selected>Conway's Game of Life</option>
                            <option value="seeds">Seeds (B2/S)</option>
                            <option value="maze">Maze (B3/S12345)</option>
                            <option value="custom">Custom Rules</option>
                        </select>
                    </div>
                    
                    <div class="control-group">
                        <label>Game Mode:</label>
                        <select id="rewardType">
                            <option value="target_practice">Target Practice</option>
                            <option value="maxwell_demon">Maxwell's Demon (Separate Left/Right)</option>
                            <option value="entropy" selected>Reduce Entropy</option>
                            <option value="complexity">Increase Complexity</option>
                            <option value="stability">Maintain Stability</option>
                        </select>
                    </div>
                </div>
            </div>
            
            <div class="mode-toggle-section">
                <div class="mode-selector">
                    <button class="mode-btn active" id="manualModeBtn">Manual Play</button>
                    <button class="mode-btn" id="rlModeBtn">RL Training</button>
                </div>
            </div>

            <!-- Manual Mode Buttons (shown by default) -->
            <div class="action-buttons" id="manualButtons">
                <!-- <button class="action-btn step-btn" id="manualStepBtn">Step CA Forward</button> -->
                <button class="action-btn reset-btn" id="resetBtn">Reset Environment</button>
                <button class="action-btn dimension-btn" id="dimensionBtn">3D View</button>
            </div>

            <!-- RL Mode Buttons (hidden by default) -->
            <div class="action-buttons" id="rlButtons" style="display: none;">
                <button class="action-btn train-btn" id="trainBtn">Start Training</button>
                <button class="action-btn step-btn" id="rlStepBtn">Single RL Step</button>
                <button class="action-btn reset-btn" id="resetRlBtn">Reset Environment</button>
                <button class="action-btn dimension-btn" id="dimensionRlBtn">3D View</button>
            </div>

            <!-- Combined Progress and Controls Row -->
            <div class="controls-section">
                <div class="combined-controls-row" style="display: flex; align-items: center; justify-content: center; gap: 20px; max-width: 100%; margin: 10px 0;">
                    
                    <!-- Progress Bar Section (hidden initially) -->
                    <div class="progress-section" id="progressSection" style="display: none; flex: 1; min-width: 200px; max-width: 350px; text-align: center;">
                        <div><strong>Episode:</strong> <span id="episodeNum">0</span> <strong>Step:</strong> <span id="stepNum">0</span></div>
                        <div class="progress-bar" style="width: 100%; height: 20px; background: #f0f0f0; border-radius: 10px; overflow: hidden; margin: 5px 0;">
                            <div class="progress-fill" id="progressFill" style="height: 100%; background: linear-gradient(90deg, #4CAF50, #45a049); width: 0%; transition: width 0.3s ease;"></div>
                        </div>
                    </div>
                
                    <!-- Manual Controls Section (shown initially) -->
                    <div class="manual-controls-section" id="manualControlsSection" style="display: flex; align-items: center; gap: 20px;">
                        <!-- Arrow Controls -->
                        <div class="manual-controls-grid" style="display: grid; grid-template-columns: repeat(3, 1fr); gap: 5px; width: 150px;">
                            <div></div>
                            <button class="action-btn up-btn" id="upBtn" style="background: #4CAF50; padding: 5px 8px; font-size: 12px;">‚Üë</button>
                            <div></div>
                            <button class="action-btn left-btn" id="leftBtn" style="background: #4CAF50; padding: 5px 8px; font-size: 12px;">‚Üê</button>
                            <button class="action-btn pass-btn" id="passBtn" style="background: #9E9E9E; padding: 5px 8px; font-size: 12px;">Pass</button>
                            <button class="action-btn right-btn" id="rightBtn" style="background: #4CAF50; padding: 5px 8px; font-size: 12px;">‚Üí</button>
                            <div></div>
                            <button class="action-btn down-btn" id="downBtn" style="background: #4CAF50; padding: 5px 8px; font-size: 12px;">‚Üì</button>
                            <div></div>
                        </div>
                        
                        <!-- Pattern Section -->
                        <div class="pattern-section" style="display: flex; align-items: center; gap: 10px;">
                            <div id="patternCanvas" class="pattern-display" style="width: 77px; height: 77px; border: 2px solid #333; position: relative; background: #fff;"></div>
                            <button class="action-btn write-btn" id="writeBtn" style="background: #FF5722; padding: 8px 12px;">Write</button>
                        </div>
                    </div>
                </div>
            </div>

            <div class="visualization-container">
                <div style="position: relative;">
                    <div id="caCanvas" class="ca-display"></div>
                    <div id="torusCanvas" class="torus-view" style="display: none;"></div>
                    <div id="agentOverlay" class="agent-overlay"></div>
                    <div id="targetOverlay" class="target-overlay" style="display: none;"></div>
                </div>
                
                <div class="stats-panel">
                    <h4>Environment:</h4>
                    <div class="metric-row">
                        <span>Entropy:</span>
                        <span id="entropy">0.0</span>
                    </div>
                    <!-- Update Note: Add additional entropy metric(s) so we cover coarse-grained, fine-grained, micro/macro state physics -->
                    <div class="metric-row" id="separationStats" style="display: none;">
                        <span>Left/Right Separation:</span>
                        <span id="separation">0.0</span>
                    </div>
                    <div class="metric-row">
                        <span>Complexity:</span>
                        <span id="complexity">0</span>
                    </div>
                    <div class="metric-row">
                        <span>Agent Position:</span>
                        <span id="agentPos">(0, 0)</span>
                    </div>
                    <h4>Reward:</h4>
                    <div class="metric-row">
                        <span>Last Reward:</span>
                        <span id="lastReward">0.0</span>
                    </div>
                    <div class="metric-row">
                        <span>Total Reward:</span>
                        <span id="totalReward">0.0</span>
                    </div>
                    <h4>Actor (Policy Network):</h4>
                    <div class="metric-row">
                        <span>Policy Entropy:</span>
                        <span id="policyEntropy">0.0</span>
                    </div>
                    <div class="metric-row">
                        <span>Action Probabilities:</span>
                    </div>
                    <div class="action-probs-chart" id="actionProbsChart"></div>
                    <div class="metric-row">
                        <span>Last Action:</span>
                        <span id="lastAction">None</span>
                    </div>

                    <div class="metric-row">
                        Last Write Pattern:
                        <!-- RL Pattern Display (hidden by default) -->
                        <div class="rl-mode-only pattern-section" style="align-items: center; gap: 10px;">
                            <div id="rlPatternCanvas" class="pattern-display" 
                                style="width: 47px; height: 47px; border: 2px solid #333; position: relative; background: #fff;">
                            </div>
                        </div>
                    </div>

                    <h4>Critic (Value Network):</h4>
                    <div class="metric-row">
                        <span>Value Estimate:</span>
                        <span id="valueEstimate">0.0</span>
                    </div>
                    <div class="metric-row">
                        <span>Advantage:</span>
                        <span id="advantage">0.0</span>
                    </div>
                    <h4>Reward:</h4>
                    <div class="metric-row">
                        <span>Learning Rate:</span>
                        <span id="learningRate">0.0001</span>
                    </div>
                    <div class="metric-row">
                        <span>Training Steps:</span>
                        <span id="trainingSteps">0</span>
                    </div>
                </div>
            </div>
            
            <div class="charts-container">
                <div class="chart-wrapper">
                    <canvas id="rewardChart" width="400" height="200"></canvas>
                </div>
            </div>

            <div class="charts-container">
                <div class="chart-wrapper">
                    <canvas id="policyGradientChart" width="400" height="200"></canvas>
                </div>
                <div class="chart-wrapper">
                    <canvas id="valueGradientChart" width="400" height="200"></canvas>
                </div>
            </div>

            <div class="separate-loss-charts">
                <div class="chart-wrapper">
                    <canvas id="policyLossChart" width="400" height="200"></canvas>
                </div>
                <div class="chart-wrapper">
                    <canvas id="valueLossChart" width="400" height="200"></canvas>
                </div>
            </div>

            <div style="text-align: left; max-width: 800px;">
                <p></p>

                <p class="intro-text">
                The Reinforcement Learning Games are comprised of a Cellular Automata grid/torus that updates according to some rule and an RL Agent that sees globally (nxn cells) but acts locally, within a 3x3 pattern/window/filter.
                </p>

                <p class="intro-text">
                Goal: An RL agent that learns to interact with cellular automata to either 1) destroy a stable 2x2 target in Game of Life, 2) reduce 'entropy' as Maxwell's Demon by separating on and off cells to the left and right 3) reduce 'entropy' by making the grid mostly on or off 4) create diverse 'complex' structures 5) maintain some balance 6) Or achieve *something*??ü§î. 
                </p>

                <h2><span class="emoji">üï∏Ô∏è & üëæ</span>RL Training Details</h2>
                <ul class="feature-list">
                <li>RL Actor Agent Policy Net üï∏Ô∏è:
                    <ul>
                    <li>Input State Tensor Shape: (n, n, 2); NxN Cellular Automata channel & NxN Agent Position channel</li>  
                    <li>Output A Distribution Over 6 Actions: move Up, Down, Left, Right, Do Nothing, Write Pattern</li>
                    <li>Current Games/Reward Functions: Choose between hitting a target, separating on/off cells, reducing entropy, increasing complexity, or maintaining stability.</li>
                    </ul>
                </li>
                <li>RL Actor Agent Pattern Net üï∏Ô∏è:
                    <ul>
                    <li>Input State Tensor Shape: (n, n, 2); NxN Cellular Automata channel & NxN Agent Position channel</li> 
                    <li>Output Pattern, 3x3 pattern, writes to CA in the event of 'Write Pattern' Action</li>
                    </ul>
                </li>
                <li>RL Critic Value Net üï∏Ô∏è:
                    <ul>
                    <li>Input State Tensor Shape: (n, n, 2); NxN Cellular Automata channel & NxN Agent Position channel</li> 
                    <li>Output State Value Scalar Estimate of the Present Value of Rewards in State S</li>
                    </ul>
                </li>
                <li>Cellular Automata Environment üëæ:
                    <ul>
                    <li>Multiple Rule Sets: Conway's Game of Life, Seeds, Maze, and custom rules</li>
                    <li>The agent is overlayed on top of the CA environment but can affect it by bit flipping or writing a 3x3 pattern</li>
                    <li>Torus topology: CA grid wraps left and right and top and bottom edges so there are no boundaries (special toroidal ConvNet padding used)</li>
                    <li>3x3 agent action region instead of single cell</li>
                    <li>Custom CA Rules: Define your own birth/survival rules</li>
                    <li>2D-3D Toggle: Switch between flat grid and torus visualization</li> <!--Update Note: Need to update so that CA is flat 2-d image on torus, not 3-D cube cells -->

                    </ul>
                </li>
                </ul>

                <h2><span class="emoji">üïπÔ∏è</span>Manual Play</h2>
                <ul class="component-list">
                <li>Choose A Game (& Study the Reward Function Using Dev Tools):
                    <ul>
                    <li>Manual Control: Use WASD keys + Space (do nothing) + G (Write Pattern)</li>
                    </ul>
                </li>
                </ul>

                <p>Dev Thought: I'm still looking for interesting, tractable, bit-flipping games for the agent to play... Got some idea?</p>
                <p>Dev Thought: Should we have multiple agents?</p>
                <p>Dev Thought: How does this relate to neural cellular automata that 'grow' and persist images they were trained on? This is weaker and can only influence one 3x3 region at a time...</p>
                <p>Dev Thought: What's the hyperparameter search plan? Or is it up to the user and give them control of the learning rates, net architectures, etc.?</p>
                <p>Dev Thought: Should we add another table to our SQL database for storing results once things are finalized? Or maybe just enable saving and loading of trained networks?</p>
                <p>Dev Thought: What 'entropy', 'complexity', 'stability' metrics do we want? What other games/metrics do we want to optimize. Maxwell's Demon: Make left side on, right side off.</p>
                <p>Dev Thought: Should we make it so the agents actions are not about moving, but it can just affect one 3x3 region at a time?</p>

            </div>
        </div>
      </div>
    </div>

    <div style="text-align: center; position: relative; top: 13px; left: 50%; transform: translateX(-50%); margin-bottom: 10px;">
        <span id="status">Ready to start</span>
    </div>
    <div style="text-align: center; position: relative; top: 13px; left: 50%; transform: translateX(-50%); margin-top: 10px;">
        <script async defer src="https://www.recurse-scout.com/loader.js?t=d879561b9052993ea69a4119aa413dbc"></script>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    
    <script>
        // Game state variables
        let targetX = 0;
        let targetY = 0;
        let hasTarget = false;
        let currentAdvantage = 0;
        let currentValueEstimate = 0;
        let currentActionProbs = [];
        let policyEntropy = 0;
        let is3DView = false;
        let torusScene = null;
        let torusRenderer = null;
        let torusCamera = null;

        // Global variables
        let caGrid = [];
        let gridSize = 12;
        let agentX = 0;
        let agentY = 0;
        let episode = 0;
        let step = 0;
        let totalReward = 0;
        let isTraining = false;

        // RL Agent variables
        let gamma = 0.95;
        let trainingSteps = 0;
        
        // let learningRate = 0.0001;
        let learningRate = 0.001;

        let policyNetwork = null;
        let valueNetwork = null;
        
        // Charts
        let rewardChart = null;
        let policyLossChart = null;
        let valueLossChart = null;
        let rewardHistory = [];
        let policyLossHistory = [];
        let valueLossHistory = [];
        let gradientHistory = [];
        // let gradientChart = null;
        let policyGradientChart = null;
        let valueGradientChart = null;
        
        const actions = ['up', 'down', 'left', 'right', 'do_nothing', 'write_pattern', ]; 
        const numActions = actions.length;
        let lastActionIdx = 0;
        let shouldWritePattern = false;

        let patternNetwork = null; // for generating the bit flip pattern when the write_patter action is taken
        let currentPattern = null;
        // let learnedPatternHistory = [];

        // Update Note: Remove old 3D functions
        let cellMeshes = [];
        let agentMesh = null;
        let animationId = null;

        // let isInRLMode = false; // Track if we're in RL mode (even when paused)
        let currentMode = 'manual'; // 'manual' or 'rl'
        // let isTraining = false;

        // Custom layer for toroidal padding
        class ToroidalPadding extends tf.layers.Layer {
            constructor() {
                super({});
            }

            call(inputs) {
                return tf.tidy(() => {
                    const input = inputs[0];
                    return addToroidalPadding(input);
                });
            }

            computeOutputShape(inputShape) {
                const [height, width, channels] = inputShape;
                return [height + 2, width + 2, channels];
            }

            getConfig() {
                const config = super.getConfig();
                return config;
            }

            static get className() {
                return 'ToroidalPadding';
            }
        }

        // Register the custom layer
        tf.serialization.registerClass(ToroidalPadding);
                
        // CA Rules
        const caRules = {
            conway: { birth: [3], survive: [2, 3] },
            seeds: { birth: [2], survive: [] },
            maze: { birth: [3], survive: [1, 2, 3, 4, 5] }
        };
        
        // Initialize everything
        window.addEventListener('load', async function() {
            initializeCharts();
            // initializeGradientChart();
            initializePolicyGradientChart(); 
            initializeValueGradientChart();  
            await initializeActorCritic();
            resetEnvironment();
            setupEventListeners();
            setupCustomRules(); // added 
            updateDisplay();
        });
        
        function initializeCharts() {
            // Reward chart
            const rewardCtx = document.getElementById('rewardChart').getContext('2d');
            rewardChart = new Chart(rewardCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'Episode Reward',
                        data: [],
                        borderColor: 'rgb(75, 192, 192)',
                        backgroundColor: 'rgba(75, 192, 192, 0.2)',
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Reward per Episode'
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: false
                        }
                    }
                }
            });
            
            // Separate policy loss chart
            const policyLossCtx = document.getElementById('policyLossChart').getContext('2d');
            policyLossChart = new Chart(policyLossCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'Policy Loss',
                        data: [],
                        borderColor: 'rgb(255, 99, 132)',
                        backgroundColor: 'rgba(255, 99, 132, 0.2)',
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Actor Policy Loss'
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true
                        }
                    }
                }
            });

            // Separate value loss chart
            const valueLossCtx = document.getElementById('valueLossChart').getContext('2d');
            valueLossChart = new Chart(valueLossCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'Value Loss',
                        data: [],
                        borderColor: 'rgb(54, 162, 235)',
                        backgroundColor: 'rgba(54, 162, 235, 0.2)',
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Critic Value Loss'
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true
                        }
                    }
                }
            });
        }

        function initializePolicyGradientChart() {
            const policyGradCtx = document.getElementById('policyGradientChart').getContext('2d');
            policyGradientChart = new Chart(policyGradCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'Policy Gradient Magnitude',
                        data: [],
                        borderColor: 'rgb(255, 206, 86)',
                        backgroundColor: 'rgba(255, 206, 86, 0.2)',
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Policy Network Gradient Magnitude'
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true
                        }
                    }
                }
            });
        }

        function initializeValueGradientChart() {
            const valueGradCtx = document.getElementById('valueGradientChart').getContext('2d');
            valueGradientChart = new Chart(valueGradCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'Value Gradient Magnitude',
                        data: [],
                        borderColor: 'rgb(153, 102, 255)',
                        backgroundColor: 'rgba(153, 102, 255, 0.2)',
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    plugins: {
                        title: {
                            display: true,
                            text: 'Value Network Gradient Magnitude'
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true
                        }
                    }
                }
            });
        }

        function initializeRlPatternCanvas() {
            const rlPatternCanvas = document.getElementById('rlPatternCanvas');
            rlPatternCanvas.innerHTML = '';

            for (let i = 0; i < 9; i++) {
                const cell = document.createElement('div');
                cell.style.position = 'absolute';
                cell.style.width = '15px';
                cell.style.height = '15px';
                cell.style.border = '1px solid #666';
                cell.style.backgroundColor = '#fff';
                cell.style.left = ((i % 3) * 15) + 'px';
                cell.style.top = (Math.floor(i / 3) * 15) + 'px';
                rlPatternCanvas.appendChild(cell);
            }
        }

        function updateRlPatternDisplay() {
            const rlPatternCanvas = document.getElementById('rlPatternCanvas');
            const cells = rlPatternCanvas.children;
            if (!currentPattern) return;
            for (let i = 0; i < 9; i++) {
                if (cells[i]) {
                    cells[i].style.backgroundColor = currentPattern[i] ? '#000' : '#fff';
                }
            }
        }

        function resetDisplayValues() {
            currentAdvantage = 0;
            currentValueEstimate = 0;
            currentActionProbs = Array(numActions).fill(1/numActions);
            policyEntropy = Math.log(numActions);
        }

        function switchToManualMode() {
            currentMode = 'manual';
            isTraining = false;
            
            // Update UI
            document.getElementById('manualModeBtn').classList.add('active');
            document.getElementById('rlModeBtn').classList.remove('active');
            document.getElementById('manualButtons').style.display = 'flex';
            document.getElementById('rlButtons').style.display = 'none';
            
            // Update combined controls section
            const combinedControls = document.querySelector('.combined-controls-row');
            // combinedControls.classList.remove('rl-mode');
            document.body.classList.remove('rl-mode');
            
            // Show manual controls, hide progress bar
            document.getElementById('manualControlsSection').style.display = 'flex';
            document.getElementById('progressSection').style.display = 'none';
            
            updateStatus('Switched to Manual Play mode');
            updatePatternDisplay();
        }

        function switchToRlMode() {
            currentMode = 'rl';
            
            // Reset training counters when switching to RL mode
            episode = 0;
            step = 0;
            totalReward = 0;
            
            // Reset environment when switching to RL mode
            resetEnvironment();
            
            // Update UI
            document.getElementById('manualModeBtn').classList.remove('active');
            document.getElementById('rlModeBtn').classList.add('active');
            document.getElementById('manualButtons').style.display = 'none';
            document.getElementById('rlButtons').style.display = 'flex';
            
            // Update combined controls section
            const combinedControls = document.querySelector('.combined-controls-row');
            // combinedControls.classList.add('rl-mode');
            document.body.classList.add('rl-mode');
            
            // Hide manual controls, show progress bar
            document.getElementById('manualControlsSection').style.display = 'none';
            document.getElementById('progressSection').style.display = 'flex';
            
            // Ensure train button shows "Start Training" for new RL session
            const trainBtn = document.getElementById('trainBtn');
            if (trainBtn) {
                trainBtn.textContent = 'Start Training';
                trainBtn.classList.remove('stop-btn');
                trainBtn.classList.add('train-btn');
            }
            
            updateDisplay();
            updateStatus('Switched to RL Training mode - episodes and steps reset');
        }

        async function startStopTraining() {
            const btn = document.getElementById('trainBtn');

            if (isTraining) {
                // Stop training
                isTraining = false;
                btn.textContent = 'Resume Training';
                btn.classList.remove('stop-btn');
                btn.classList.add('train-btn');
                updateStatus(`Training paused at Episode ${episode}, Step ${step}`);
            } else {
                // Start/Resume training
                isTraining = true;
                btn.textContent = 'Stop Training';
                btn.classList.remove('train-btn');
                btn.classList.add('stop-btn');
                
                const isResuming = step > 0;
                updateStatus(isResuming ? 'Training resumed' : 'Training started');

                // Run continuous episodes until stopped
                while (isTraining) {
                    await runEpisode();
                    // Small delay between episodes for UI responsiveness
                    if (isTraining) {
                        await new Promise(resolve => setTimeout(resolve, 100));
                    }
                }
            }
        }

        // Function to apply manual toroidal padding (because ConvNets look locally and the CA grid is on a torus that wraps around on itself)
        // We don't want normal blank padding for the edges of the ConvNet because the border cells aren't really true borders.
        function addToroidalPadding(inputTensor) {
            const [batchSize, height, width, channels] = inputTensor.shape;
            
            // Pad top and bottom
            const topRow = inputTensor.slice([0, height - 1, 0, 0], [batchSize, 1, width, channels]);
            const bottomRow = inputTensor.slice([0, 0, 0, 0], [batchSize, 1, width, channels]);
            const verticalPadded = tf.concat([topRow, inputTensor, bottomRow], 1);
            
            // Pad left and right
            const leftColumn = verticalPadded.slice([0, 0, width - 1, 0], [batchSize, height + 2, 1, channels]);
            const rightColumn = verticalPadded.slice([0, 0, 0, 0], [batchSize, height + 2, 1, channels]);
            const fullyPadded = tf.concat([leftColumn, verticalPadded, rightColumn], 2);
            
            return fullyPadded;
        }

        async function singleRlStep() {
            if (isTraining) {
                updateStatus('Cannot step manually while training is active');
                return;
            }
            
            // Execute one RL step with training
            const state = getState();
            const action = selectAction(state);
            const reward = executeAction(action);
            const nextState = getState();
            
            totalReward += reward;
            step++;
            
            // Train the networks on this step
            await trainActorCritic(state, action, reward, nextState, false);
            
            updateDisplay();
            updateStatus(`RL step completed. Action: ${actions[action]}, Reward: ${reward.toFixed(3)}`);
            
            // Clean up tensors
            state.dispose();
            nextState.dispose();
        }

        async function initializeActorCritic() {
            // Input shape: [gridSize, gridSize, 2];  CA grid with toroidal padding + agent position channel
            const inputShape = [gridSize + 2, gridSize + 2, 2];
            // Input shape: [batchSize, gridSize, gridSize, 2]
            // const inputShape = [null, gridSize, gridSize, 2];
            
            // Safely dispose existing networks
            try {
                if (policyNetwork) policyNetwork.dispose();
                if (valueNetwork) valueNetwork.dispose();  
                if (patternNetwork) patternNetwork.dispose();
            } catch (e) {
                // Networks already disposed
            }
            
            policyNetwork = tf.sequential({
                layers: [
                    tf.layers.inputLayer({ inputShape: inputShape }),
                    tf.layers.conv2d({ filters: 32, kernelSize: 3, activation: 'relu', padding: 'valid' }),
                    tf.layers.conv2d({ filters: 64, kernelSize: 3, activation: 'relu', padding: 'same' }),
                    tf.layers.flatten(),
                    tf.layers.dense({ units: 128, activation: 'relu' }),
                    tf.layers.dense({ units: numActions, activation: 'softmax' })
                ]
            });

            policyNetwork.compile({
                optimizer: tf.train.adam(learningRate),
                loss: 'categoricalCrossentropy'
            });

            // Pattern network: ConvNet that outputs 3x3 pattern
            patternNetwork = tf.sequential({
                layers: [
                    // tf.layers.inputLayer({ inputShape: inputShape }),
                    tf.layers.conv2d({ 
                        inputShape: inputShape, 
                        filters: 16, 
                        kernelSize: 3, 
                        activation: 'relu',
                        padding: 'same'
                    }),
                    tf.layers.conv2d({ 
                        filters: 32, 
                        kernelSize: 3, 
                        activation: 'relu',
                        padding: 'same'
                    }),
                    tf.layers.flatten(),
                    tf.layers.dense({ units: 64, activation: 'relu' }),
                    tf.layers.dense({ units: 9, activation: 'sigmoid' }) // 3x3 = 9 outputs
                ]
            });
            
            patternNetwork.compile({
                optimizer: tf.train.adam(learningRate),
                loss: 'binaryCrossentropy'
            });

            // Value network: ConvNet that outputs V(s)
            valueNetwork = tf.sequential({
                layers: [
                    // tf.layers.inputLayer({ inputShape: inputShape }),
                    tf.layers.conv2d({ 
                        inputShape: inputShape, 
                        filters: 32, 
                        kernelSize: 3, 
                        activation: 'relu',
                        padding: 'same'
                    }),
                    tf.layers.conv2d({ 
                        filters: 64, 
                        kernelSize: 3, 
                        activation: 'relu',
                        padding: 'same'
                    }),
                    // tf.layers.globalAveragePooling2d(),
                    tf.layers.flatten(),
                    tf.layers.dense({ units: 128, activation: 'relu' }),
                    tf.layers.dense({ units: 1 })
                ]
            });
            
            valueNetwork.compile({
                optimizer: tf.train.adam(learningRate),
                loss: 'meanSquaredError'
            });

            // Initialize display values
            resetDisplayValues();
        }

        function calculateSeparationReward() {
            const leftSide = [];
            const rightSide = [];
            const midpoint = Math.floor(gridSize / 2);
            
            // Split grid into left and right halves
            for (let i = 0; i < gridSize; i++) {
                for (let j = 0; j < midpoint; j++) {
                    leftSide.push(caGrid[i][j]);
                }
                for (let j = midpoint; j < gridSize; j++) {
                    rightSide.push(caGrid[i][j]);
                }
            }
            
            const leftDensity = leftSide.reduce((sum, cell) => sum + cell, 0) / leftSide.length;
            const rightDensity = rightSide.reduce((sum, cell) => sum + cell, 0) / rightSide.length;
            
            // Reward for maximizing difference between sides
            // Perfect separation would be leftDensity = 1, rightDensity = 0 (or vice versa)
            const separation = Math.abs(leftDensity - rightDensity);
            return separation * 10; // Scale the reward
        }

        function visualizeSeparation() {
            if (is3DView) return; // Skip for 3D view for now
            
            const canvas = document.getElementById('caCanvas');
            const cellSize = 10;
            const midpoint = Math.floor(gridSize / 2);
            
            // Add a visual separator line
            let separator = document.getElementById('separatorLine');
            if (!separator) {
                separator = document.createElement('div');
                separator.id = 'separatorLine';
                separator.style.position = 'absolute';
                separator.style.backgroundColor = '#ffff00';
                separator.style.zIndex = '10';
                separator.style.pointerEvents = 'none';
                canvas.appendChild(separator);
            }
            
            separator.style.left = (midpoint * cellSize - 1) + 'px';
            separator.style.top = '0px';
            separator.style.width = '2px';
            separator.style.height = (gridSize * cellSize) + 'px';
            separator.style.display = 'block';
        }

        function hideSeparationVisuals() {
            const separator = document.getElementById('separatorLine');
            if (separator) {
                separator.style.display = 'none';
            }
        }

        function resetRlEnvironment() {
            console.log("resetRl");
            // Stop training first
            // if (isTraining) {
            //     isTraining = false;
            //     const trainBtn = document.getElementById('trainBtn');
            //     if (trainBtn) {
            //         console.log("yes on trainBtn");
            //         trainBtn.textContent = 'Start Training';
            //         trainBtn.classList.remove('stop-btn');
            //         trainBtn.classList.add('train-btn');
            //     } 
            // }

            isTraining = false;
            const trainBtn = document.getElementById('trainBtn');
            if (trainBtn) {
                console.log("yes on trainBtn");
                trainBtn.textContent = 'Start Training';
                trainBtn.classList.remove('stop-btn');
                trainBtn.classList.add('train-btn');
            } 
            
            // Reset everything including training progress
            episode = 0;
            step = 0;
            totalReward = 0;
            trainingSteps = 0;

            // Clear training history
            rewardHistory = [];
            policyLossHistory = [];
            valueLossHistory = [];
            gradientHistory = [];
            
            // Clear charts
            if (rewardChart) {
                rewardChart.data.labels = [];
                rewardChart.data.datasets[0].data = [];
                rewardChart.update();
            }
            
            if (policyLossChart) {
                policyLossChart.data.labels = [];
                policyLossChart.data.datasets[0].data = [];
                policyLossChart.update();
            }
            
            if (valueLossChart) {
                valueLossChart.data.labels = [];
                valueLossChart.data.datasets[0].data = [];
                valueLossChart.update();
            }
            
            if (policyGradientChart) {
                policyGradientChart.data.labels = [];
                policyGradientChart.data.datasets[0].data = [];
                policyGradientChart.update();
            }
            
            if (valueGradientChart) {
                valueGradientChart.data.labels = [];
                valueGradientChart.data.datasets[0].data = [];
                valueGradientChart.update();
            }
            
            // Call the regular reset environment
            resetEnvironment();
            
            updateStatus('RL environment and training progress completely reset');
        }

        function setupEventListeners() {
            // Mode toggle buttons
            document.getElementById('manualModeBtn').addEventListener('click', switchToManualMode);
            document.getElementById('rlModeBtn').addEventListener('click', switchToRlMode);
            
            // Manual mode buttons
            // document.getElementById('manualStepBtn').addEventListener('click', manualStepCA);
            document.getElementById('resetBtn').addEventListener('click', resetEnvironment);
            document.getElementById('dimensionBtn').addEventListener('click', toggle3DView);
            
            // RL mode buttons
            document.getElementById('trainBtn').addEventListener('click', startStopTraining);
            document.getElementById('rlStepBtn').addEventListener('click', singleRlStep);
            // document.getElementById('resetRlBtn').addEventListener('click', resetEnvironment);
            document.getElementById('resetRlBtn').addEventListener('click', resetRlEnvironment);
            document.getElementById('dimensionRlBtn').addEventListener('click', toggle3DView);
            
            // Grid size change
            document.getElementById('gridSize').addEventListener('change', function() {
                const newGridSize = parseInt(this.value);
                if (newGridSize !== gridSize) {
                    gridSize = newGridSize;
                    
                    // Dispose old networks if they exist
                    if (policyNetwork) policyNetwork.dispose();
                    if (valueNetwork) valueNetwork.dispose();
                    if (patternNetwork) patternNetwork.dispose();
                    
                    // Reinitialize networks with new state size
                    initializeActorCritic();
                    resetEnvironment();
                }
            });
            
            document.getElementById('initialDensity').addEventListener('change', resetEnvironment);
            document.getElementById('caRules').addEventListener('change', resetEnvironment);
            document.getElementById('rewardType').addEventListener('change', function() {
                handleGameModeChange();
                resetEnvironment();
            });

            // Manual control buttons (for manual mode only)
            document.getElementById('upBtn').addEventListener('click', () => executeManualAction(0));
            document.getElementById('downBtn').addEventListener('click', () => executeManualAction(1));
            document.getElementById('leftBtn').addEventListener('click', () => executeManualAction(2));
            document.getElementById('rightBtn').addEventListener('click', () => executeManualAction(3));
            document.getElementById('passBtn').addEventListener('click', () => executeManualAction(4));
            document.getElementById('writeBtn').addEventListener('click', () => executeManualAction(5));

            document.addEventListener('keydown', function(event) {
                console.log("keydown", event);
                if (currentMode !== 'manual' || isTraining) return;
                
                let action = -1;
                switch(event.key) {
                    case 'ArrowUp':
                    case 'w':
                    case 'W':
                        action = 0;
                        break;
                    case 'ArrowDown':
                    case 's':
                    case 'S':
                        action = 1;
                        break;
                    case 'ArrowLeft':
                    case 'a':
                    case 'A':
                        action = 2;
                        break;
                    case 'ArrowRight':
                    case 'd':
                    case 'D':
                        action = 3;
                        break;
                    case ' ':
                        action = 4; // do nothing (advance CA)
                        event.preventDefault();
                        break;
                    case 'g':
                    case 'G':
                        action = 5; // write pattern
                        break;
                }
                
                if (action !== -1) {
                    executeManualAction(action);
                    event.preventDefault();
                }
            });
            
            // Initialize pattern canvas
            initializePatternCanvas();
            initializeRlPatternCanvas();
        }

        function executeManualAction(action) {
            if (currentMode !== 'manual') return;
            
            const state = getState();
            
            // For write action, use the manual pattern interface pattern
            if (action === 5) {
                if (!currentPattern) {
                    currentPattern = new Array(9).fill(0);
                }
            }
            
            const reward = executeAction(action);
            totalReward += reward;

            // Log state, action, reward for dataset building (simulated to console here)
            console.log("Episode Log:", {
                // state: caGrid.map(row => [...row]), // shallow copy of CA state
                state: getState().arraySync()[0], //n x n x 2
                action: actions[action],
                pattern: action === 5? currentPattern: null,
                reward: reward
            });

            step++;
            updateDisplay();
            updateStatus(`Manual action: ${actions[action]}, Reward: ${reward.toFixed(3)}`);
            
            state.dispose();
        }

        function initializePatternCanvas() {
            const patternCanvas = document.getElementById('patternCanvas');
            patternCanvas.innerHTML = '';
            
            // Create 3x3 grid of cells
            for (let i = 0; i < 9; i++) {
                const cell = document.createElement('div');
                cell.style.position = 'absolute';
                cell.style.width = '25px';
                cell.style.height = '25px';
                cell.style.border = '1px solid #666';
                cell.style.backgroundColor = '#fff';
                cell.style.left = ((i % 3) * 25) + 'px';
                cell.style.top = (Math.floor(i / 3) * 25) + 'px';
                cell.style.cursor = 'pointer';
                cell.dataset.index = i;
                
                // Make cells clickable for manual pattern editing
                cell.addEventListener('click', function() {
                    const index = parseInt(this.dataset.index);
                    if (!currentPattern) currentPattern = new Array(9).fill(0);
                    currentPattern[index] = 1 - currentPattern[index];
                    updatePatternDisplay();
                });
                
                patternCanvas.appendChild(cell);
            }
            
            // Initialize pattern
            currentPattern = new Array(9).fill(0);
            updatePatternDisplay();
        }

        function updatePatternDisplay() {
            const patternCanvas = document.getElementById('patternCanvas');
            const cells = patternCanvas.children;
            
            if (!currentPattern) return;
            
            for (let i = 0; i < 9; i++) {
                if (cells[i]) {
                    cells[i].style.backgroundColor = currentPattern[i] ? '#000' : '#fff';
                }
            }
        }

        function resetEnvironment() {
            // Handle game mode specific setup
            handleGameModeChange();
            
            // Initialize grid
            const density = parseFloat(document.getElementById('initialDensity').value || '0.4');
            caGrid = [];
            
            // For target practice, start with empty grid
            if (document.getElementById('rewardType').value === 'target_practice') {
                for (let i = 0; i < gridSize; i++) {
                    caGrid[i] = [];
                    for (let j = 0; j < gridSize; j++) {
                        caGrid[i][j] = 0; // Empty grid
                    }
                }
            } else {
                // For other games, use random density
                for (let i = 0; i < gridSize; i++) {
                    caGrid[i] = [];
                    for (let j = 0; j < gridSize; j++) {
                        caGrid[i][j] = Math.random() < density ? 1 : 0;
                    }
                }
            }
            
            // Reset agent position
            agentX = Math.floor(gridSize / 2);
            agentY = Math.floor(gridSize / 2);

            // Initialize target if in target mode
            if (document.getElementById('rewardType').value === 'target_practice') {
                spawnTarget();
            } else {
                hasTarget = false;
                document.getElementById('targetOverlay').style.display = 'none';
            }
            
            // Only reset episode/step stats when in manual mode OR when called by resetRlEnvironment
            if (currentMode === 'manual') {
                step = 0;
                episode = 0;
                totalReward = 0;
                
                // Stop training if active (manual mode shouldn't have active training)
                const trainBtn = document.getElementById('trainBtn');
                if (trainBtn && isTraining) {
                    isTraining = false;
                    trainBtn.textContent = 'Start Training';
                    trainBtn.classList.remove('stop-btn');
                    trainBtn.classList.add('train-btn');
                }
            }
            // In RL mode, only reset the environment, not the training progress
            
            updateDisplay();
            
            // Only show reset message when not in active training
            if (!isTraining) {
                updateStatus('Environment reset');
            }
        }

        // Optional: Add a function to exit RL mode back to manual mode
        function exitRLMode() {
            if (isTraining) return; // Don't allow if currently training
            
            isInRLMode = false;
            const progressSection = document.getElementById('progressSection');
            const manualControlsSection = document.getElementById('manualControlsSection');
            const trainBtn = document.getElementById('trainBtn');
            
            manualControlsSection.style.display = 'flex';
            progressSection.style.display = 'none';
            trainBtn.textContent = 'Start RL Training';
            trainBtn.classList.remove('action-btn');
            trainBtn.classList.add('train-btn');
            
            updateStatus('Switched to manual mode');
        }

        // Update toggle function to properly handle cleanup
        function toggle3DView() {
            const btn = document.getElementById('dimensionBtn');
            const canvas2d = document.getElementById('caCanvas');
            const canvas3d = document.getElementById('torusCanvas');
            
            is3DView = !is3DView;
            
            if (is3DView) {
                btn.textContent = '2D View';
                canvas2d.style.display = 'none';
                canvas3d.style.display = 'block';
                initTorus3D();
            } else {
                btn.textContent = '3D View';
                canvas2d.style.display = 'block';
                canvas3d.style.display = 'none';
                
                // Cleanup 3D resources
                if (animationId) {
                    cancelAnimationFrame(animationId);
                    animationId = null;
                }
                if (torusRenderer && canvas3d.contains(torusRenderer.domElement)) {
                    canvas3d.removeChild(torusRenderer.domElement);
                }
            }
            updateDisplay();
        }

        function initializeCellMeshes() {
            // Clear existing meshes
            cellMeshes.forEach(mesh => {
                torusScene.remove(mesh);
                mesh.geometry.dispose();
                mesh.material.dispose();
            });
            cellMeshes = [];

            const radius = gridSize / (2 * Math.PI);
            const cellSize = 0.6;
            
            // Create shared geometries for performance
            const aliveGeometry = new THREE.PlaneGeometry(cellSize, cellSize);
            const deadGeometry = new THREE.PlaneGeometry(cellSize * 0.3, cellSize * 0.3);
            
            const aliveMaterial = new THREE.MeshLambertMaterial({ color: 0x00ff00 });
            const deadMaterial = new THREE.MeshLambertMaterial({ 
                color: 0x002200, 
                transparent: true, 
                opacity: 0.3 
            });

            // Pre-create all cell meshes
            for (let i = 0; i < gridSize; i++) {
                for (let j = 0; j < gridSize; j++) {
                    const mesh = new THREE.Mesh(aliveGeometry, aliveMaterial);
                    
                    // Position on torus
                    const phi = (i / gridSize) * 2 * Math.PI;
                    const theta = (j / gridSize) * 2 * Math.PI;
                    
                    const x = (radius + 2 * Math.cos(theta)) * Math.cos(phi);
                    const y = (radius + 2 * Math.cos(theta)) * Math.sin(phi);
                    const z = 2 * Math.sin(theta);

                    mesh.position.set(x, y, z);
                    
                    // Orient the plane to face outward from torus
                    mesh.lookAt(x * 2, y * 2, z * 2);
                    
                    cellMeshes.push(mesh);
                    torusScene.add(mesh);
                }
            }

            // Create agent mesh
            if (agentMesh) {
                torusScene.remove(agentMesh);
                agentMesh.geometry.dispose();
                agentMesh.material.dispose();
            }
            
            const agentGeometry = new THREE.BoxGeometry(cellSize * 1.5, cellSize * 1.5, 0.2);
            const agentMaterial = new THREE.MeshLambertMaterial({ color: 0xff0000 });
            agentMesh = new THREE.Mesh(agentGeometry, agentMaterial);
            torusScene.add(agentMesh);
        }

        function initTorus3D() {
            const container = document.getElementById('torusCanvas');
            const width = gridSize * 15;
            const height = gridSize * 15;

            // Clean up existing scene
            if (torusRenderer) {
                container.removeChild(torusRenderer.domElement);
                cancelAnimationFrame(animationId);
            }

            // Scene
            torusScene = new THREE.Scene();
            torusScene.background = new THREE.Color(0x111111);

            // Camera
            torusCamera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
            torusCamera.position.set(gridSize * 1.2, gridSize * 1.2, gridSize * 1.2);
            torusCamera.lookAt(0, 0, 0);

            // Renderer with performance settings
            torusRenderer = new THREE.WebGLRenderer({ 
                antialias: false,  // Disable for performance
                powerPreference: "high-performance"
            });
            torusRenderer.setSize(width, height);
            torusRenderer.setPixelRatio(Math.min(window.devicePixelRatio, 2)); // Limit pixel ratio
            container.appendChild(torusRenderer.domElement);

            // Lights
            const ambientLight = new THREE.AmbientLight(0x404040, 0.6);
            torusScene.add(ambientLight);

            const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
            directionalLight.position.set(gridSize, gridSize, gridSize);
            torusScene.add(directionalLight);

            // Create torus wireframe (static)
            const torusGeometry = new THREE.TorusGeometry(gridSize / (2 * Math.PI) + 2, 1, 8, gridSize);
            const wireframeMaterial = new THREE.MeshBasicMaterial({ 
                color: 0x444444, 
                wireframe: true,
                transparent: true,
                opacity: 0.2
            });
            const torusWireframe = new THREE.Mesh(torusGeometry, wireframeMaterial);
            torusScene.add(torusWireframe);

            // Pre-create all cell meshes for reuse
            initializeCellMeshes();
            
            render3D();
        }
        
        function render3D() {
            if (!is3DView || !torusRenderer) return;

            // Update cell appearances without recreating meshes
            const radius = gridSize / (2 * Math.PI);
            const cellSize = 0.6;
            
            for (let i = 0; i < gridSize; i++) {
                for (let j = 0; j < gridSize; j++) {
                    const meshIndex = i * gridSize + j;
                    const mesh = cellMeshes[meshIndex];
                    if (mesh) {
                        const isAlive = caGrid[i][j] === 1;
                        mesh.material.color.setHex(isAlive ? 0x00ff00 : 0x002200);
                        mesh.material.opacity = isAlive ? 1.0 : 0.3;
                        mesh.scale.setScalar(isAlive ? 1.0 : 0.5);
                    }
                }
            }

            // Update agent position
            if (agentMesh) {
                const phi = (agentY / gridSize) * 2 * Math.PI;
                const theta = (agentX / gridSize) * 2 * Math.PI;
                
                const x = (radius + 2.5 * Math.cos(theta)) * Math.cos(phi);
                const y = (radius + 2.5 * Math.cos(theta)) * Math.sin(phi);
                const z = 2.5 * Math.sin(theta);

                agentMesh.position.set(x, y, z);
            }

            // Static camera (remove rotation for performance)
            // Optional: uncomment for slow rotation
            const time = Date.now() * 0.0002;
            torusCamera.position.x = Math.cos(time) * gridSize * 1.5;
            torusCamera.position.z = Math.sin(time) * gridSize * 1.5;
            torusCamera.lookAt(0, 0, 0);

            torusRenderer.render(torusScene, torusCamera);
            
            // Use requestAnimationFrame only when in 3D view
            if (is3DView) {
                animationId = requestAnimationFrame(render3D);
            }
        }

        function updateDisplay() {
            if (is3DView) {
                render3D();
            } else {
                // Update 2D CA grid display
                const canvas = document.getElementById('caCanvas');
                const cellSize = 10;
                canvas.style.width = (gridSize * cellSize) + 'px';
                canvas.style.height = (gridSize * cellSize) + 'px';

                canvas.style.position = 'relative';
                canvas.style.margin = '0 auto';
                canvas.style.display = 'block';

                // Ensure parent container centers content
                const visualContainer = canvas.closest('.visualization-container');
                if (visualContainer) {
                    visualContainer.style.alignItems = 'center';
                }
                
                // Clear and redraw
                canvas.innerHTML = '';
                
                // Create cells
                for (let i = 0; i < gridSize; i++) {
                    for (let j = 0; j < gridSize; j++) {
                        const cell = document.createElement('div');
                        cell.style.position = 'absolute';
                        cell.style.left = (j * cellSize) + 'px';
                        cell.style.top = (i * cellSize) + 'px';
                        cell.style.width = cellSize + 'px';
                        cell.style.height = cellSize + 'px';
                        cell.style.backgroundColor = caGrid[i][j] ? '#000' : '#fff';
                        cell.style.border = '1px solid #ccc';
                        canvas.appendChild(cell);
                    }
                }
                
                // Update agent position - now shows 3x3 region
                const agentOverlay = document.getElementById('agentOverlay');
                agentOverlay.style.left = ((agentX - 1) * cellSize) + 'px';
                agentOverlay.style.top = ((agentY - 1) * cellSize) + 'px';
                agentOverlay.style.width = (3 * cellSize) + 'px';
                agentOverlay.style.height = (3 * cellSize) + 'px';
            }
            
            // Update stats
            updateStats();

            // added
            // Update episode & step numbers
            document.getElementById('episodeNum').textContent = episode;
            document.getElementById('stepNum').textContent = step;

            // Update progress bar fill
            // Assume a fixed max steps per episode (e.g., 200)
            const maxStepsPerEpisode = 200; 
            const progressPercent = Math.min((step / maxStepsPerEpisode) * 100, 100);
            document.getElementById('progressFill').style.width = progressPercent + '%';
        }

        function updateActionProbsChart() {
            const chartContainer = document.getElementById('actionProbsChart');
            chartContainer.innerHTML = '';
            
            currentActionProbs.forEach((prob, index) => {
                const bar = document.createElement('div');
                bar.className = 'action-bar';
                bar.style.width = (prob * 100) + '%';
                bar.style.backgroundColor = `hsl(${index * 60}, 70%, 50%)`;
                bar.setAttribute('data-action', actions[index]);
                chartContainer.appendChild(bar);
            });
        }
        
        function updateStats() {
            const livingCells = countLivingCells();
            const entropy = calculateEntropy();
            const complexity = calculateComplexity();
            
            document.getElementById('entropy').textContent = entropy.toFixed(3);
            // document.getElementById('complexity').textContent = complexity.toFixed(3);
            document.getElementById('complexity').textContent = complexity;
            document.getElementById('agentPos').textContent = `(${agentX}, ${agentY})`;
            document.getElementById('episodeNum').textContent = episode;
            document.getElementById('stepNum').textContent = step;
            document.getElementById('totalReward').textContent = totalReward.toFixed(2);
            
            // Actor-Critic specific metrics
            document.getElementById('policyEntropy').textContent = policyEntropy.toFixed(3);
            document.getElementById('valueEstimate').textContent = currentValueEstimate.toFixed(2);
            document.getElementById('advantage').textContent = currentAdvantage.toFixed(3);
            document.getElementById('trainingSteps').textContent = trainingSteps;

            const lastActionElement = document.getElementById('lastAction');
            lastActionElement.textContent = actions[lastActionIdx];
            lastActionElement.style.color = `hsl(${lastActionIdx * 60}, 70%, 50%)`;

            const rewardType = document.getElementById('rewardType').value;
            const separationStats = document.getElementById('separationStats')
                        
            updateActionProbsChart();
            
            // Update target if in target mode
            if (rewardType === 'target_practice') {
                updateTargetDisplay();
            } else if (rewardType === 'maxwell_demon') {
                if (separationStats) {
                    separationStats.style.display = 'block';
                    const separation = calculateSeparationReward() / 10; // Unscale for display
                    document.getElementById('separation').textContent = separation.toFixed(3);
                }
                visualizeSeparation();
                document.getElementById('targetOverlay').style.display = 'none';
            } else {
                if (separationStats) {
                    separationStats.style.display = 'none';
                }
                hideSeparationVisuals();
                document.getElementById('targetOverlay').style.display = 'none';
            }
        }
        
        function countLivingCells() {
            let count = 0;
            for (let i = 0; i < gridSize; i++) {
                for (let j = 0; j < gridSize; j++) {
                    if (caGrid[i][j]) count++;
                }
            }
            return count;
        }
        
        function calculateEntropy() {
            const living = countLivingCells();
            const total = gridSize * gridSize;
            const p = living / total;
            if (p === 0 || p === 1) return 0;
            return -(p * Math.log2(p) + (1 - p) * Math.log2(1 - p));
        }
        
        function calculateComplexity() {
            // Measure local patterns diversity (simplified complexity measure)
            let patterns = new Set();
            for (let i = 1; i < gridSize - 1; i++) {
                for (let j = 1; j < gridSize - 1; j++) {
                    let pattern = '';
                    for (let di = -1; di <= 1; di++) {
                        for (let dj = -1; dj <= 1; dj++) {
                            pattern += caGrid[i + di][j + dj];
                        }
                    }
                    patterns.add(pattern);
                }
            }
            // return patterns.size / 512; // Normalize by max possible 3x3 patterns
            return patterns.size; // Normalize by max possible 3x3 patterns
        }

        function spawnTarget() {
            // Create a 2x2 stable block pattern as target
            let attempts = 0;
            do {
                targetX = Math.floor(Math.random() * (gridSize - 2));
                targetY = Math.floor(Math.random() * (gridSize - 2));
                attempts++;
            } while (attempts < 50 && (Math.abs(targetX - agentX) < 4 || Math.abs(targetY - agentY) < 4));
            
            // Write a 2x2 block
            caGrid[targetY][targetX] = 1;
            caGrid[targetY][targetX + 1] = 1;
            caGrid[targetY + 1][targetX] = 1;
            caGrid[targetY + 1][targetX + 1] = 1;
            
            hasTarget = true;
            updateTargetDisplay();
        }

        function updateTargetDisplay() {
            const targetOverlay = document.getElementById('targetOverlay');
            const cellSize = 10;
            
            if (hasTarget && !is3DView) {
                targetOverlay.style.display = 'block';
                targetOverlay.style.left = (targetX * cellSize) + 'px';
                targetOverlay.style.top = (targetY * cellSize) + 'px';
                targetOverlay.style.width = (2 * cellSize) + 'px';
                targetOverlay.style.height = (2 * cellSize) + 'px';
            } else {
                targetOverlay.style.display = 'none';
            }
        }

        function checkTargetDestroyed() {
            if (!hasTarget) return false;
            
            // Check if 2x2 block is destroyed
            const intact = caGrid[targetY][targetX] === 1 && 
                        caGrid[targetY][targetX + 1] === 1 && 
                        caGrid[targetY + 1][targetX] === 1 && 
                        caGrid[targetY + 1][targetX + 1] === 1;
            
            if (!intact) {
                hasTarget = false;
                updateTargetDisplay();
                return true;
            }
            return false;
        }

        function getState() {
            // Create two-channel input: [gridSize, gridSize, 2]
            const stateArray = [];
            for (let i = 0; i < gridSize; i++) {
                const row = [];
                for (let j = 0; j < gridSize; j++) {
                    const inAgentRegion = Math.abs(i - agentY) <= 1 && Math.abs(j - agentX) <= 1;
                    row.push([caGrid[i][j], inAgentRegion ? 1 : 0]);
                }
                stateArray.push(row);
            }
            
            // Convert to tensor and apply toroidal padding
            const stateTensor = tf.tensor3d(stateArray).expandDims(0); // Add batch dimension
            const paddedTensor = addToroidalPadding(stateTensor);
            
            return paddedTensor; // Return tensor instead of flattened array
        }

        function selectAction(state) {
            // Get action probabilities from policy network
            const probs = policyNetwork.predict(state);
            const probsData = probs.dataSync();
            
            // Store for display
            currentActionProbs = Array.from(probsData);
            policyEntropy = -currentActionProbs.reduce((sum, p) => sum + p * Math.log(p + 1e-8), 0);
            
            // Get value estimate
            const value = valueNetwork.predict(state);
            currentValueEstimate = value.dataSync()[0];
            
            // Get pattern from pattern network
            const patternProbs = patternNetwork.predict(state);
            const patternData = patternProbs.dataSync();
            currentPattern = Array.from(patternData).map(p => p > 0.5 ? 1 : 0);
            
            const action = weightedRandomChoice(probsData);
            
            // Cleanup
            probs.dispose();
            value.dispose();
            patternProbs.dispose();
            
            return action;
        }

        function weightedRandomChoice(weights) {
            let sum = 0;
            const r = Math.random();
            for (let i = 0; i < weights.length; i++) {
                sum += weights[i];
                if (r < sum) return i;
            }
            return weights.length - 1;
        }

        function executeAction(action) {
            let reward = 0;
            const oldEntropy = calculateEntropy();
            const oldComplexity = calculateComplexity();
            
            // Move agent based on action
            switch (action) {
                case 0: // up
                    agentY = (agentY - 1 + gridSize) % gridSize; // Torus wrapping
                    break;
                case 1: // down
                    agentY = (agentY + 1) % gridSize; // Torus wrapping
                    break;
                case 2: // left
                    agentX = (agentX - 1 + gridSize) % gridSize; // Torus wrapping
                    break;
                case 3: // right
                    agentX = (agentX + 1) % gridSize; // Torus wrapping
                    break;
                case 4: // do nothing
                    break;
                case 5: // write learned pattern
                    shouldWritePattern = true;
                    break;
            }
            
            // Calculate reward based on game mode
            const rewardType = document.getElementById('rewardType').value;
            
            // Update CA first
            updateCA();
            
            // Then calculate reward after CA update
            const newEntropy = calculateEntropy();
            const newComplexity = calculateComplexity();
            // Update Note: should we add new entropy measures here?
            
            switch (rewardType) {
                case 'target_practice':
                    if (checkTargetDestroyed()) {
                        reward += 100;
                        updateStatus('Target destroyed! +100 reward');
                        
                        // Clear all living cells
                        for (let i = 0; i < gridSize; i++) {
                            for (let j = 0; j < gridSize; j++) {
                                caGrid[i][j] = 0;
                            }
                        }
                        
                        // Spawn new target immediately
                        spawnTarget();
                    }
                    reward -= 0.1;
                    break;
                case 'entropy':
                    reward += (oldEntropy - newEntropy) * 10;
                    break;
                case 'complexity':
                    reward += (newComplexity - oldComplexity) * 10;
                    break;
                case 'stability':
                    reward += Math.max(0, 1 - Math.abs(newEntropy - oldEntropy)) * 5;
                    break;
                case 'maxwell_demon':
                    reward += calculateSeparationReward();
                    visualizeSeparation();
                    break;
            }
            
            // Update display
            lastActionIdx = action;
            // console.log(lastActionIdx, 'lai');

            document.getElementById('lastReward').textContent = reward.toFixed(3);
            
            return reward;
        }

        function writeLearnedPattern() {
            // Add null check
            if (!currentPattern || currentPattern.length !== 9) {
                console.warn("Invalid pattern, skipping write operation");
                return;
            }

            // Apply the learned 3x3 pattern at agent position
            for (let i = 0; i < 3; i++) {
                for (let j = 0; j < 3; j++) {
                    const x = (agentX + j - 1 + gridSize) % gridSize;
                    const y = (agentY + i - 1 + gridSize) % gridSize;
                    const patternIdx = i * 3 + j;
                    caGrid[y][x] = currentPattern[patternIdx];
                }
            }
        }

        // Handle dropdown visibility for Target Game mode
        function handleGameModeChange() {
            const rewardType = document.getElementById('rewardType').value;
            const densityGroup = document.querySelector('.control-group:has(#initialDensity)');
            const caRulesSelect = document.getElementById('caRules');
            
            if (rewardType === 'target_practice') {
                // Hide initial density for target mode
                if (densityGroup) {
                    densityGroup.style.display = 'none';
                }
                // Lock to Conway's Game of Life
                caRulesSelect.value = 'conway';
                caRulesSelect.disabled = true;
                hideSeparationVisuals();
            } else if (rewardType === 'maxwell_demon') {
                // Show density control for Maxwell's Demon
                if (densityGroup) {
                    densityGroup.style.display = 'flex';
                }
                caRulesSelect.disabled = false;
                // Don't call visualizeSeparation here - it will be called during executeAction
            } else {
                // Show initial density for other modes
                if (densityGroup) {
                    densityGroup.style.display = 'flex';
                }
                caRulesSelect.disabled = false;
                hideSeparationVisuals();
            }
        }

        function updateCA() {
            const newGrid = caGrid.map(row => row.slice());
            const ruleSet = document.getElementById('caRules').value;
            const rules = caRules[ruleSet] || caRules.conway;
            
            for (let i = 0; i < gridSize; i++) {
                for (let j = 0; j < gridSize; j++) {
                    const neighbors = countNeighbors(i, j);
                    const currentState = caGrid[i][j];
                    
                    if (currentState === 0) {
                        // Dead cell - check birth rules
                        if (rules.birth.includes(neighbors)) {
                            newGrid[i][j] = 1;
                        }
                    } else {
                        // Living cell - check survival rules
                        if (!rules.survive.includes(neighbors)) {
                            newGrid[i][j] = 0;
                        }
                    }
                }
            }

            caGrid = newGrid;

            if (shouldWritePattern) {
                writeLearnedPattern();
                updateRlPatternDisplay();
                shouldWritePattern = false;
            }
        }
        
        function countNeighbors(x, y) {
            let count = 0;
            for (let i = -1; i <= 1; i++) {
                for (let j = -1; j <= 1; j++) {
                    if (i === 0 && j === 0) continue;
                    // Use torus topology for neighbor counting
                    const nx = (x + i + gridSize) % gridSize;
                    const ny = (y + j + gridSize) % gridSize;
                    count += caGrid[nx][ny];
                }
            }
            return count;
        }
        
        function updateRewardChart() {
            rewardChart.data.labels.push(episode);
            rewardChart.data.datasets[0].data.push(totalReward);
            rewardChart.update();
        }
        
        function updateLossCharts() {
            if (policyLossHistory.length > 0) {
                const latestPolicy = policyLossHistory[policyLossHistory.length - 1];
                policyLossChart.data.labels.push(trainingSteps);
                policyLossChart.data.datasets[0].data.push(latestPolicy);
                policyLossChart.update();
            }
            
            if (valueLossHistory.length > 0) {
                const latestValue = valueLossHistory[valueLossHistory.length - 1];
                valueLossChart.data.labels.push(trainingSteps);
                valueLossChart.data.datasets[0].data.push(latestValue);
                valueLossChart.update();
            }

            if (gradientHistory.length > 0) {
                const latestGradients = gradientHistory[gradientHistory.length - 1];
                
                // Update policy gradient chart
                if (policyGradientChart) {
                    policyGradientChart.data.labels.push(trainingSteps);
                    policyGradientChart.data.datasets[0].data.push(latestGradients.policyGradMag || 0);
                    
                    // Keep only last 100 data points for performance
                    if (policyGradientChart.data.labels.length > 100) {
                        policyGradientChart.data.labels.shift();
                        policyGradientChart.data.datasets[0].data.shift();
                    }
                    
                    policyGradientChart.update('none'); // No animation for better performance
                }
                
                // Update value gradient chart
                if (valueGradientChart) {
                    valueGradientChart.data.labels.push(trainingSteps);
                    valueGradientChart.data.datasets[0].data.push(latestGradients.valueGradMag || 0);
                    
                    // Keep only last 100 data points for performance
                    if (valueGradientChart.data.labels.length > 100) {
                        valueGradientChart.data.labels.shift();
                        valueGradientChart.data.datasets[0].data.shift();
                    }
                    
                    valueGradientChart.update('none'); // No animation for better performance
                }
            }
        }

        async function runEpisode() {
            const maxStepsPerEpisode = 200;
            
            // Only initialize new episode if starting fresh (step = 0)
            if (step === 0) {
                episode++;
                totalReward = 0;
                updateStatus(`Starting Episode ${episode}`);
            }

            // Continue from current step until episode end or training stops
            while (step < maxStepsPerEpisode && isTraining) {
                const state = getState();
                const action = selectAction(state);
                const reward = executeAction(action);

                totalReward += reward;
                step++;

                const nextState = getState();
                const isLastStep = (step >= maxStepsPerEpisode);
                await trainActorCritic(state, action, reward, nextState, isLastStep);

                state.dispose();
                nextState.dispose();

                updateDisplay();
                
                // Use tf.nextFrame() for better performance and UI responsiveness
                await tf.nextFrame();
            }

            // Episode completed (not paused)
            if (step >= maxStepsPerEpisode && isTraining) {
                // Store episode reward for chart
                rewardHistory.push(totalReward);
                if (rewardChart) {
                    rewardChart.data.labels.push(episode);
                    rewardChart.data.datasets[0].data.push(totalReward);
                    rewardChart.update('none'); // No animation for better performance
                }
                
                updateStatus(`Episode ${episode} completed with reward ${totalReward.toFixed(3)}`);
                
                // Reset for next episode
                step = 0;
                totalReward = 0;
                resetEnvironment();
            }
        }

        async function toggleTraining() {
            const btn = document.getElementById('trainBtn');
            const progressSection = document.getElementById('progressSection');
            const manualControlsSection = document.getElementById('manualControlsSection');
            
            if (isTraining) {
                // Stop training but stay in RL mode
                isTraining = false;
                isInRLMode = true; // Keep RL mode active
                btn.textContent = 'Resume Training';
                btn.classList.remove('action-btn');
                btn.classList.add('train-btn');
                
                // Keep progress bar visible, hide manual controls
                manualControlsSection.style.display = 'none';
                progressSection.style.display = 'flex';
                
                updateStatus('Training paused - use "RL Step & Stop" to continue manually');
            } else if (isInRLMode) {
                // Resume training from paused state
                isTraining = true;
                btn.textContent = 'Stop Training';
                btn.classList.remove('train-btn');
                btn.classList.add('action-btn');
                
                updateStatus('Training resumed');
                
                // Continue training loop
                while (isTraining) {
                    await runEpisode();
                    if (isTraining) {
                        await new Promise(resolve => setTimeout(resolve, 500));
                    }
                }
            } else {
                // Start training from manual mode
                isTraining = true;
                isInRLMode = true;
                btn.textContent = 'Stop Training';
                btn.classList.remove('train-btn');
                btn.classList.add('action-btn');
                
                // Hide manual controls, show progress bar
                manualControlsSection.style.display = 'none';
                progressSection.style.display = 'flex';
                
                updateStatus('Training started');
                
                // Run continuous episodes
                while (isTraining) {
                    await runEpisode();
                    if (isTraining) {
                        await new Promise(resolve => setTimeout(resolve, 500));
                    }
                }
            }
        }

        async function rlStep() {
            if (isTraining) return; // Don't allow manual step during training
            
            if (!isInRLMode) {
                // If not in RL mode, switch to RL mode and show progress bar
                isInRLMode = true;
                const progressSection = document.getElementById('progressSection');
                const manualControlsSection = document.getElementById('manualControlsSection');
                manualControlsSection.style.display = 'none';
                progressSection.style.display = 'flex';
                
                // Update train button to show resume option
                const btn = document.getElementById('trainBtn');
                btn.textContent = 'Resume Training';
                btn.classList.remove('action-btn');
                btn.classList.add('train-btn');
            }
            
            // Execute one RL step with training
            const state = getState();
            const action = selectAction(state);
            const reward = executeAction(action);
            const nextState = getState();
            
            totalReward += reward;
            step++;
            
            // Train the networks on this step
            await trainActorCritic(state, action, reward, nextState, false);
            
            updateDisplay();
            updateStatus(`Action: ${actions[action]}, Reward: ${reward.toFixed(3)}`);
            
            // Clean up tensors
            state.dispose();
            nextState.dispose();
        }
        
        function updateStatus(message) {
            document.getElementById('status').textContent = message;
            console.log(message);
        }

        async function trainActorCritic(state, action, reward, nextState, done) {
            // const stateTensor = tf.tensor2d([state]);
            // const nextStateTensor = tf.tensor2d([nextState]);
            const stateTensor = state; // state is already a tensor
            const nextStateTensor = nextState; // nextState is already a tensor
            
            // Get current and next state values
            const currentValue = valueNetwork.predict(stateTensor);
            const nextValue = done ? tf.zeros([1, 1]) : valueNetwork.predict(nextStateTensor);
            
            const currentValueData = currentValue.dataSync()[0];
            const nextValueData = nextValue.dataSync()[0];
            
            // Calculate TD target and advantage with clipping
            const tdTarget = reward + gamma * nextValueData;
            const advantage = Math.max(-5, Math.min(5, tdTarget - currentValueData));
            
            currentAdvantage = advantage;
            
            // Train value network and track gradient
            const targetTensor = tf.tensor2d([[tdTarget]]);
            let valueGradMagnitude = 0;
            
            // Calculate value network gradients
            const valueGrads = tf.variableGrads(() => {
                const pred = valueNetwork.apply(stateTensor);
                return tf.losses.meanSquaredError(targetTensor, pred);
            });
            
            // Calculate gradient magnitude for value network
            valueGradMagnitude = Object.values(valueGrads.grads).reduce((sum, grad) => {
                return sum + tf.norm(grad).dataSync()[0];
            }, 0);
            
            const valueFitHistory = await valueNetwork.fit(stateTensor, targetTensor, { 
                epochs: 1, 
                verbose: 0,
                validationSplit: 0 
            });
            const valueLoss = valueFitHistory.history.loss[0];
            
            // Train policy network with gradient tracking
            const actionTensor = tf.tensor1d([action], 'int32');
            let policyLossValue = 0;
            let policyGradMagnitude = 0;
            
            const optimizer = tf.train.adam(learningRate * 0.1); // Update Note: should we remove 0.1? what is the purpose of adjusting learning rate here?
            
            const f = () => tf.tidy(() => {
                const logits = policyNetwork.apply(stateTensor);
                const probs = tf.softmax(logits);
                const logProbs = tf.log(tf.clipByValue(probs, 1e-8, 1 - 1e-8));
                const actionMask = tf.oneHot(actionTensor, numActions);
                const pickedLogProb = tf.sum(tf.mul(logProbs, actionMask));
                
                const clippedAdvantage = tf.clipByValue(tf.scalar(advantage), -5, 5);
                const loss = tf.neg(tf.mul(pickedLogProb, clippedAdvantage));
                
                policyLossValue = loss.dataSync()[0];
                return loss;
            });
            
            // Calculate policy gradients before optimization
            const policyGrads = tf.variableGrads(f);
            policyGradMagnitude = Object.values(policyGrads.grads).reduce((sum, grad) => {
                return sum + tf.norm(grad).dataSync()[0];
            }, 0);
            
            await optimizer.minimize(f);
            optimizer.dispose();

            if (action === 5) { // If write pattern action was taken
                // Use advantage as feedback for pattern network training
                // Positive advantage = good pattern, negative advantage = bad pattern
                
                // Create target pattern based on advantage
                let targetPattern;
                if (advantage > 0) {
                    // Reinforce current pattern if advantage is positive
                    targetPattern = tf.tensor2d([currentPattern.map(x => x)]);
                } else {
                    // If advantage is negative, target opposite of current pattern
                    targetPattern = tf.tensor2d([currentPattern.map(x => 1 - x)]);
                }
                
                // Scale learning based on advantage magnitude
                const patternLearningRate = Math.abs(advantage) * 0.01; // Update Note: do we want hard coded learning rates?
                const patternOptimizer = tf.train.adam(patternLearningRate);
                
                // const patternLoss = () => {
                //     const pred = patternNetwork.apply(stateTensor);
                //     return tf.losses.binaryCrossentropy(targetPattern, pred);
                // };
                const patternLoss = () => {
                    const pred = patternNetwork.apply(stateTensor);
                    return tf.metrics.binaryCrossentropy(targetPattern, pred).mean();
                };
                                
                await patternOptimizer.minimize(patternLoss);
                
                // Cleanup
                targetPattern.dispose();
                patternOptimizer.dispose();
            }
                        
            // Store gradient information
            gradientHistory.push({
                step: trainingSteps,
                policyGradMag: policyGradMagnitude,
                valueGradMag: valueGradMagnitude,
                policyLoss: policyLossValue,
                valueLoss: valueLoss
            });
            
            // Keep gradient history manageable
            if (gradientHistory.length > 1000) {
                gradientHistory.shift();
            }
            
            // Store losses with NaN check
            if (!isNaN(policyLossValue) && !isNaN(valueLoss)) {
                policyLossHistory.push(policyLossValue);
                valueLossHistory.push(valueLoss);
                updateLossCharts();
            }
            
            trainingSteps++;
            
            // Cleanup tensors
            Object.values(valueGrads.grads).forEach(grad => grad.dispose());
            Object.values(policyGrads.grads).forEach(grad => grad.dispose());
            // stateTensor.dispose();
            // nextStateTensor.dispose();
            currentValue.dispose();
            nextValue.dispose();
            targetTensor.dispose();
            actionTensor.dispose();
        }

        function setupCustomRules() {
            const rulesSelect = document.getElementById('caRules');
            
            // Create the interface immediately but hide it
            createCustomRulesInterface();
            
            rulesSelect.addEventListener('change', function() {
                const customRulesDiv = document.getElementById('customRules');
                if (this.value === 'custom') {
                    if (customRulesDiv) {
                        customRulesDiv.style.display = 'block';
                    }
                } else {
                    if (customRulesDiv) {
                        customRulesDiv.style.display = 'none';
                    }
                }
            });
        }
        
        function createCustomRulesInterface() {
            const container = document.querySelector('.controls-section');
            const customDiv = document.createElement('div');
            customDiv.id = 'customRules';
            customDiv.className = 'control-group';
            customDiv.style.display = 'none';
            customDiv.style.background = '#1f3f27';
            customDiv.style.padding = '10px';
            customDiv.style.borderRadius = '5px';
            customDiv.style.margin = '10px 0';
            
            customDiv.innerHTML = `
                <label>Birth Rule (neighbor count for dead cell to become alive):</label>
                <div style="display: flex; gap: 5px; margin: 5px 0;">
                    ${[0,1,2,3,4,5,6,7,8].map(n => 
                        `<label style="font-size: 11px;"><input type="checkbox" id="birth${n}" value="${n}"> ${n}</label>`
                    ).join('')}
                </div>
                <label>Survival Rule (neighbor count for live cell to stay alive):</label>
                <div style="display: flex; gap: 5px; margin: 5px 0;">
                    ${[0,1,2,3,4,5,6,7,8].map(n => 
                        `<label style="font-size: 11px;"><input type="checkbox" id="survive${n}" value="${n}"> ${n}</label>`
                    ).join('')}
                </div>
            `;
            
            container.appendChild(customDiv);
            
            // Set default Conway rules
            document.getElementById('birth3').checked = true;
            document.getElementById('survive2').checked = true;
            document.getElementById('survive3').checked = true;
            
            // Update rules when checkboxes change
            customDiv.addEventListener('change', updateCustomRules);
        }
        
        function updateCustomRules() {
            const birth = [];
            const survive = [];
            
            for (let i = 0; i <= 8; i++) {
                const birthCheck = document.getElementById(`birth${i}`);
                const surviveCheck = document.getElementById(`survive${i}`);
                
                if (birthCheck && birthCheck.checked) birth.push(i);
                if (surviveCheck && surviveCheck.checked) survive.push(i);
            }
            
            caRules.custom = { birth, survive };
        }

        window.addEventListener('beforeunload', function() {
            if (policyNetwork) policyNetwork.dispose();
            if (valueNetwork) valueNetwork.dispose();
            if (patternNetwork) patternNetwork.dispose();
        });
        
        console.log('Reinforcement Learning Cellular Automata System Initialized!');
    </script>
</body>
</html>